{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from brainlm_mae.modeling_brainlm import BrainLMForPretraining\n",
    "from utils.brainlm_trainer import BrainLMTrainer\n",
    "from utils.plots import plot_future_timepoint_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2023-07-17-19_00_00\"\n",
    "checkpoint_n = \"500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type brainlm_mae to instantiate a model of type vit_mae. This is not supported for all configurations of models and can yield errors.\n",
      "C:\\Users\\liy106\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BrainLMForPretraining(\n",
       "  (vit): BrainLMModel(\n",
       "    (embeddings): BrainLMEmbeddings(\n",
       "      (patch_embeddings): None\n",
       "      (signal_embedding_projection): Linear(in_features=20, out_features=512, bias=True)\n",
       "      (xyz_embedding_projection): Linear(in_features=3, out_features=512, bias=True)\n",
       "      (pos_embedding): PositionalEncoding(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder): BrainLMEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x NystromformerLayer(\n",
       "          (attention): NystromformerAttention(\n",
       "            (self): NystromformerSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (conv): Conv2d(4, 4, kernel_size=(65, 1), stride=(1, 1), padding=(32, 0), groups=4, bias=False)\n",
       "            )\n",
       "            (output): NystromformerSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): NystromformerIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): NystromformerOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BrainLMDecoder(\n",
       "    (decoder_embed): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-1): 2 x NystromformerLayer(\n",
       "        (attention): NystromformerAttention(\n",
       "          (self): NystromformerSelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (conv): Conv2d(4, 4, kernel_size=(65, 1), stride=(1, 1), padding=(32, 0), groups=4, bias=False)\n",
       "          )\n",
       "          (output): NystromformerSelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): NystromformerIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): NystromformerOutput(\n",
       "          (dense): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (decoder_xyz_projection): Linear(in_features=3, out_features=512, bias=True)\n",
       "    (pos_embedding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder_pred1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (decoder_pred_nonlinearity): LeakyReLU(negative_slope=0.1)\n",
       "    (decoder_pred2): Linear(in_features=256, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BrainLMForPretraining.from_pretrained(\n",
    "#     f\"/home/ahf38/palmer_scratch/brainlm/training-runs/{model_name}/checkpoint-{checkpoint_n}\").to(device)\n",
    "# model\n",
    "\n",
    "model = BrainLMForPretraining.from_pretrained(\n",
    "    f\"C:\\yamin\\eeg-fmri-DL\\BrainLM\\pretrained_models\\checkpoint\").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTMAEConfig {\n",
      "  \"_name_or_path\": \"C:\\\\yamin\\\\eeg-fmri-DL\\\\BrainLM\\\\pretrained_models\\\\checkpoint\",\n",
      "  \"architectures\": [\n",
      "    \"BrainLMForPretraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"conv_kernel_size\": 65,\n",
      "  \"decoder_hidden_size\": 512,\n",
      "  \"decoder_intermediate_size\": 1024,\n",
      "  \"decoder_num_attention_heads\": 4,\n",
      "  \"decoder_num_hidden_layers\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 512,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"inv_coeff_init_option\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"loss_fn\": \"mse\",\n",
      "  \"mask_ratio\": 0.2,\n",
      "  \"max_eval_samples\": 800,\n",
      "  \"model_type\": \"vit_mae\",\n",
      "  \"norm_pix_loss\": false,\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_brain_voxels\": 424,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_landmarks\": 64,\n",
      "  \"num_timepoints_per_voxel\": 200,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"segment_means_seq_len\": 64,\n",
      "  \"timepoint_patching_size\": 20,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.39.0.dev0\",\n",
      "  \"use_tanh_decoder\": false,\n",
      "  \"weight_decay\": 1e-05\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.vit.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(model.vit.embeddings.mask_ratio)\n",
    "print(model.vit.embeddings.config.mask_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this for matteo's branch, due to multiple train modes (auto-encoder, causal attention, predict last, etc)\n",
    "model.config.train_mode = \"auto_encode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Load Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Index', 'X', 'Y', 'Z'],\n",
      "    num_rows: 424\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "coords_ds = load_from_disk(\"C:\\\\yamin\\\\eeg-fmri-DL\\\\BrainLM\\\\toolkit\\\\sample_dataset\\\\a424_fMRI_data\\\\arrow_form\\\\Brain_Region_Coordinates\")\n",
    "print(coords_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_v = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Raw_Recording', 'Voxelwise_RobustScaler_Normalized_Recording', 'All_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_Per_Voxel_Normalized_Recording', 'Per_Voxel_All_Patient_Normalized_Recording', 'Subtract_Mean_Normalized_Recording', 'Subtract_Mean_Divide_Global_STD_Normalized_Recording', 'Subtract_Mean_Divide_Global_99thPercent_Normalized_Recording', 'Filename', 'Patient ID'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load all data\n",
    "train_ds = load_from_disk(\"C:\\\\yamin\\\\eeg-fmri-DL\\\\BrainLM\\\\toolkit\\\\sample_dataset\\\\a424_fMRI_data\\\\arrow_form\\\\train\")\n",
    "print(train_ds)\n",
    "# val_ds = load_from_disk(\"/home/sr2464/palmer_scratch/datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/val_ukbiobank\")\n",
    "# print(val_ds)\n",
    "# test_ds = load_from_disk(\"/home/sr2464/palmer_scratch/datasets/UKB_Large_rsfMRI_and_tffMRI_Arrow_WithRegression_v3_with_metadata/test_ukbiobank\")\n",
    "# print(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vcon02-scan01_EPI2MNI_sm_nr.dat\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0]['Filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_ds = concatenate_datasets([train_ds, val_ds, test_ds])\n",
    "concat_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example0 = test_ds[500]\n",
    "print(example0['Filename'])\n",
    "print(example0['Patient ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = f\"C:\\\\yamin\\\\eeg-fmri-DL\\\\BrainLM\\\\inference_plots\\\\dataset_{dataset_v}/{model_name}_ckpt-{checkpoint_n}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = {\"train\": train_ds, \"val\": val_ds, \"test\": test_ds, \"concat\": concat_ds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Through Model, Pass Whole fMRI Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable_of_interest_col_name = \"Gender\"\n",
    "variable_of_interest_col_name = \"\"\n",
    "recording_col_name = \"Voxelwise_RobustScaler_Normalized_Recording\"\n",
    "length = 200\n",
    "num_timepoints_per_voxel = model.config.num_timepoints_per_voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fmri(examples):\n",
    "    \"\"\"\n",
    "    Preprocessing function for dataset samples. This function is passed into Trainer as\n",
    "    a preprocessor which takes in one row of the loaded dataset and constructs a model\n",
    "    input sample according to the arguments which model.forward() expects.\n",
    "\n",
    "    The reason this function is defined inside on main() function is because we need\n",
    "    access to arguments such as cell_expression_vector_col_name.\n",
    "    \"\"\"\n",
    "    #\n",
    "    signal_vector = examples[recording_col_name][0]\n",
    "    signal_vector = torch.tensor(signal_vector, dtype=torch.float32)\n",
    "\n",
    "    # Choose random starting index, take window of moving_window_len points for each region\n",
    "    start_idx = randint(0, signal_vector.shape[0] - num_timepoints_per_voxel)\n",
    "    end_idx = start_idx + num_timepoints_per_voxel\n",
    "    signal_window = signal_vector[\n",
    "        start_idx:end_idx, :\n",
    "    ]  # [moving_window_len, num_voxels]\n",
    "    signal_window = torch.movedim(\n",
    "        signal_window, 0, 1\n",
    "    )  # --> [num_voxels, moving_window_len]\n",
    "\n",
    "    # Append signal values and coords\n",
    "    window_xyz_list = []\n",
    "    for brain_region_idx in range(signal_window.shape[0]):\n",
    "        # window_timepoint_list = torch.arange(0.0, 1.0, 1.0 / num_timepoints_per_voxel)\n",
    "\n",
    "        # Append voxel coordinates\n",
    "        xyz = torch.tensor(\n",
    "            [\n",
    "                coords_ds[brain_region_idx][\"X\"],\n",
    "                coords_ds[brain_region_idx][\"Y\"],\n",
    "                coords_ds[brain_region_idx][\"Z\"],\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        window_xyz_list.append(xyz)\n",
    "    window_xyz_list = torch.stack(window_xyz_list)\n",
    "\n",
    "    # Add in key-value pairs for model inputs which CellLM is expecting in forward() function:\n",
    "    #  signal_vectors and xyzt_vectors\n",
    "    #  These lists will be stacked into torch Tensors by collate() function (defined above).\n",
    "    examples[\"signal_vectors\"] = [torch.stack(signal_val_list, dim=0)]\n",
    "    examples[\"xyzt_vectors\"] = [torch.stack(xyzt_list, dim=0)]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    This function tells the dataloader how to stack a batch of examples from the dataset.\n",
    "    Need to stack gene expression vectors and maintain same argument names for model inputs\n",
    "    which CellLM is expecting in forward() function:\n",
    "        expression_vectors, sampled_gene_indices, and cell_indices\n",
    "    \"\"\"\n",
    "    signal_vectors = torch.stack(\n",
    "        [example[\"signal_vectors\"] for example in examples], dim=0\n",
    "    )\n",
    "    xyz_vectors = torch.stack([example[\"xyz_vectors\"] for example in examples])\n",
    "    labels = torch.stack([example[\"label\"] for example in examples])\n",
    "    \n",
    "    \n",
    "    # These inputs will go to model.forward(), names must match\n",
    "    return {\n",
    "        \"signal_vectors\": signal_vectors,\n",
    "        \"xyz_vectors\": xyz_vectors,\n",
    "        \"input_ids\": signal_vectors,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_ds.set_transform(preprocess_fmri)\n",
    "# test_ds.set_transform(preprocess_fmri)\n",
    "train_ds.set_transform(preprocess_fmri)\n",
    "# val_ds.set_transform(preprocess_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_single = DataLoader(train_ds, batch_size=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Raw_Recording', 'Voxelwise_RobustScaler_Normalized_Recording', 'All_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_All_Voxel_Normalized_Recording', 'Per_Patient_Per_Voxel_Normalized_Recording', 'Per_Voxel_All_Patient_Normalized_Recording', 'Subtract_Mean_Normalized_Recording', 'Subtract_Mean_Divide_Global_STD_Normalized_Recording', 'Subtract_Mean_Divide_Global_99thPercent_Normalized_Recording', 'Filename', 'Patient ID'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 424 is out of bounds for size 424",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_single\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\arrow_dataset.py:2807\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2806\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2807\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2808\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\arrow_dataset.py:2788\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2786\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2787\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2788\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2790\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\formatting\\formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\formatting\\formatting.py:515\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    513\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m    514\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[1;32m--> 515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 32\u001b[0m, in \u001b[0;36mpreprocess_fmri\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     25\u001b[0m window_xyz_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m brain_region_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(signal_window\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# window_timepoint_list = torch.arange(0.0, 1.0, 1.0 / num_timepoints_per_voxel)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Append voxel coordinates\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     xyz \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     31\u001b[0m         [\n\u001b[1;32m---> 32\u001b[0m             \u001b[43mcoords_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbrain_region_idx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     33\u001b[0m             coords_ds[brain_region_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     34\u001b[0m             coords_ds[brain_region_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     35\u001b[0m         ],\n\u001b[0;32m     36\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m     37\u001b[0m     )\n\u001b[0;32m     38\u001b[0m     window_xyz_list\u001b[38;5;241m.\u001b[39mappend(xyz)\n\u001b[0;32m     39\u001b[0m window_xyz_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(window_xyz_list)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\arrow_dataset.py:2787\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2786\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2787\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2788\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2790\u001b[0m )\n\u001b[0;32m   2791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\formatting\\formatting.py:583\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    582\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m--> 583\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch_deeplearning\\lib\\site-packages\\datasets\\formatting\\formatting.py:526\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[1;34m(key, size)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[1;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: Invalid key: 424 is out of bounds for size 424"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloader_single)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader_single))[\"signal_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Forward 1 sample through just the model encoder (model.vit) ---#\n",
    "with torch.no_grad():\n",
    "    example1 = next(iter(dataloader_single))\n",
    "    encoder_output = model.vit(\n",
    "        signal_vectors=example1[\"signal_vectors\"].to(device),\n",
    "        xyz_vectors=example1[\"xyz_vectors\"].to(device),\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"last_hidden_state:\", encoder_output.last_hidden_state.shape)\n",
    "# [batch_size, num_genes + 1 CLS token, hidden_dim]\n",
    "\n",
    "cls_token = encoder_output.last_hidden_state[:,0,:]\n",
    "print(cls_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete reconstruction of a batch of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_batched = DataLoader(dataset_split[split], \n",
    "                               batch_size=batch_size, \n",
    "                               num_workers=6, \n",
    "                               collate_fn=collate_fn,\n",
    "                               pin_memory=True,\n",
    "                               drop_last=False,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_noise(x, seq_length, mask_ratio, ids_mask=None):\n",
    "    \"\"\"\n",
    "    Constructs a noise tensor which is used by model.vit.embeddings to mask tokens.\n",
    "    Giving ids_mask enables that every new call of construct_noise will return a tensor \n",
    "    that masks tokens that were not previously masked.\n",
    "\n",
    "    Args:\n",
    "        x:              tensor of shape [batch_size, num_voxels, num_timepoints_per_voxel]\n",
    "        seq_length:     length of tokens\n",
    "        mask_ratio:     ratio of tokens to mask\n",
    "        ids_mask:       previously masked tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # label dimensions of interest\n",
    "    batch_size = x.shape[0]\n",
    "    len_mask = int(mask_ratio * seq_length)\n",
    "    \n",
    "    # construct random only for not previously masked tokens\n",
    "    # add zeros to noise to force keep previously masked tokens\n",
    "    noise = torch.rand(batch_size, seq_length, device=x.device)\n",
    "    \n",
    "    if ids_mask != None:        \n",
    "        # force keep by setting noise to zero at prior masked indeces\n",
    "        noise.scatter_(index = ids_mask, dim = 1, value=0)\n",
    "\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "    \n",
    "        ids_mask = torch.cat((ids_mask, ids_shuffle[:, (-1 * len_mask):]), dim=1)\n",
    "    else:\n",
    "        ids_mask = torch.argsort(noise, dim=1)[:, (-1 * len_mask):]  # ascend: small is keep, large is remove\n",
    "    \n",
    "    return noise, ids_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reconstruct whole recording for one example by repeatedly encoding/decoding\n",
    "example1 = next(iter(dataloader_batched))\n",
    "seq_length = (model.config.num_timepoints_per_voxel // model.config.timepoint_patching_size) * model.config.num_brain_voxels\n",
    "\n",
    "masked_tokens = 0\n",
    "ids_removed = None\n",
    "predictions = []\n",
    "masks = []\n",
    "x=example1[\"signal_vectors\"].to(device)\n",
    "while masked_tokens < seq_length:\n",
    "    \n",
    "    noise, ids_removed = construct_noise(x, seq_length, model.config.mask_ratio, ids_removed)\n",
    "    \n",
    "    # get predictions\n",
    "    out = model(signal_vectors=x, \n",
    "                xyz_vectors=example1[\"xyz_vectors\"].to(device),\n",
    "                noise=noise,\n",
    "               )\n",
    "    \n",
    "    # store predictions and masks\n",
    "    predictions.append(out.logits[0].detach())\n",
    "    print(\"Masked tokens this run at sample 0 and parcel 0:\", torch.nonzero(out.mask[0, 0, :]).tolist())\n",
    "    masks.append(out.mask)\n",
    "\n",
    "    \n",
    "    masked_tokens += out[\"mask\"][0, :].sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictions(predictions, masks, mode=\"first\"):\n",
    "    '''\n",
    "    Aggregates all predictions according to masks.\n",
    "    Avoids adding a prediction twice if masked twice (will happen if num_tokens % (masked_ratio * num_tokens) != 0) \n",
    "    by taking the first prediction if mode = \"first\", or the average if mode = \"mean\". \n",
    "    '''\n",
    "    preds = torch.zeros(predictions[0].shape)\n",
    "    cum_mask = torch.zeros(masks[0].shape) # counts how many times particular token is masked\n",
    "    \n",
    "    for idx, p in enumerate(predictions):\n",
    "        cum_mask += masks[idx]\n",
    "        \n",
    "        if mode == \"first\":\n",
    "            masked_once = (cum_mask == 1) # keeps only tokens masked once\n",
    "            m = torch.eq(masked_once.long(), masks[idx]) # returns True for unmasked tokens and tokens masked for first time\n",
    "            m = m.long() * masks[idx] # returns only tokens masked for first time (mask[idx] will be zero for others)\n",
    "        else:\n",
    "            m = masks[idx]\n",
    "            \n",
    "        m = m.unsqueeze(-1).repeat(1, 1, 1, p.shape[-1])\n",
    "\n",
    "        preds += p * m \n",
    "\n",
    "    if mode == \"mean\":\n",
    "        preds = preds / cum_mask.unsqueeze(-1).repeat(1, 1, 1, p.shape[-1]) # there should not be a division by zero because all tokens masked at least once\n",
    "    \n",
    "    return (preds, cum_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, cum_mask = aggregate_predictions(predictions, masks, mode=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting one sample\n",
    "\n",
    "Visualize wholly reconstructed recording and ground truth. Plot UMAP and PCA over space as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcol\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_masked_pred_trends_one_sample(\n",
    "    pred_logits: np.array,\n",
    "    signal_vectors: np.array,\n",
    "    mask: np.array,\n",
    "    sample_idx: int,\n",
    "    node_idxs: np.array,\n",
    "    dataset_split: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to plot timeseries of model predictions as continuation of input data compared to\n",
    "    ground truth.\n",
    "    Args:\n",
    "        pred_logits:    numpy array of shape [batch_size, num_voxels, num_tokens, time_patch_preds]\n",
    "        signal_vectors: numpy array of shape [batch_size, num_voxels, num_tokens, time_patch_preds]\n",
    "        mask:           binary mask of shape [batch_size, num_voxels, num_tokens]\n",
    "        sample_idx:     index of sample to plot; one per figure\n",
    "        node_idxs:      indices of voxels to plot; affects how many columns in plot grid there will be\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=len(node_idxs), ncols=1, sharex=True, sharey=True)\n",
    "    fig.set_figwidth(25)\n",
    "    fig.set_figheight(3 * len(node_idxs))\n",
    "\n",
    "    batch_size, num_voxels, num_tokens, time_patch_preds = pred_logits.shape\n",
    "\n",
    "    # --- Plot Figure ---#\n",
    "    for row_idx, node_idx in enumerate(node_idxs):\n",
    "        ax = axes[row_idx]\n",
    "\n",
    "        input_data_vals = []\n",
    "        input_data_timepoints = []\n",
    "        for token_idx in range(signal_vectors.shape[2]):\n",
    "            input_data_vals += signal_vectors[sample_idx, node_idx, token_idx].tolist()\n",
    "            start_timepoint = time_patch_preds * token_idx\n",
    "            end_timepoint = start_timepoint + time_patch_preds\n",
    "            input_data_timepoints += list(range(start_timepoint, end_timepoint))\n",
    "\n",
    "            if mask[sample_idx, node_idx, token_idx] == 1:\n",
    "                model_pred_vals = pred_logits[sample_idx, node_idx, token_idx].tolist()\n",
    "                model_pred_timepoints = list(range(start_timepoint, end_timepoint))\n",
    "                ax.plot(\n",
    "                    model_pred_timepoints,\n",
    "                    model_pred_vals,\n",
    "                    marker=\".\",\n",
    "                    markersize=3,\n",
    "                    label=\"Masked Predictions\",\n",
    "                    color=\"orange\",\n",
    "                )\n",
    "\n",
    "        ax.plot(\n",
    "            input_data_timepoints,\n",
    "            input_data_vals,\n",
    "            marker=\".\",\n",
    "            markersize=3,\n",
    "            label=\"Input Data\",\n",
    "            color=\"green\",\n",
    "        )\n",
    "        ax.set_title(\"Sample {}, Parcel {}\".format(sample_idx, node_idx))\n",
    "        ax.axhline(y=0.0, color=\"gray\", linestyle=\"--\", markersize=2)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.9, 0.99))\n",
    "    plt.tight_layout(rect=[0.03, 0.03, 0.95, 0.95])\n",
    "    fig.supxlabel(\"Timepoint\")\n",
    "    fig.supylabel(\"Prediction Value\")\n",
    "    plt.suptitle(\"Ground Truth Signal vs Masked Prediction\\n({} Split)\".format(dataset_split))\n",
    "    plt.savefig(f\"{dir_name}reconstruct_whole_recording_{dataset_split}split.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_masked_pred_trends_one_sample(\n",
    "    preds,\n",
    "    example1[\"signal_vectors\"].reshape(preds.shape),\n",
    "    mask=torch.ones(preds.shape[:3]),\n",
    "    sample_idx=0,\n",
    "    node_idxs=[0, 100, 200],\n",
    "    dataset_split=split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot UMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one recording to map, transpose to do PCA and UMAP over time\n",
    "raw_rec = example1[\"signal_vectors\"][0].T\n",
    "pred_rec = preds[0].flatten(-2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply UMAP to raw recording\n",
    "reducer = umap.UMAP(random_state=42, verbose = True, n_components=3)\n",
    "embedding_raw = reducer.fit_transform(raw_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP to reconstructed recording\n",
    "reducer = umap.UMAP(random_state=42, verbose = True, n_components=3)\n",
    "embedding_pred = reducer.fit_transform(pred_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=embedding_pred.shape[1], ncols=1, sharex=True, sharey=False)\n",
    "fig.set_figwidth(25)\n",
    "fig.set_figheight(3 * embedding_pred.shape[1])\n",
    "\n",
    "# batch_size, num_voxels, num_tokens, time_patch_preds = pred_logits.shape\n",
    "\n",
    "# --- Plot Figure ---#\n",
    "for row_idx in range(embedding_pred.shape[1]):\n",
    "    ax = axes[row_idx]\n",
    "    data_parcels = list(range(embedding_pred.shape[0]))\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        embedding_pred[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Masked Predictions\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        embedding_raw[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Input Data\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.set_title(\"UMAP Coord {}\".format(row_idx))\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", bbox_to_anchor=(0.5, 0.9))\n",
    "plt.tight_layout(rect=[0.03, 0.03, 0.95, 0.95])\n",
    "fig.supxlabel(\"Timepoint\")\n",
    "fig.supylabel(\"UMAP value\")\n",
    "plt.suptitle(\"Ground Truth Signal vs Masked Prediction\\n({} Split)\".format(split))\n",
    "plt.savefig(f\"{dir_name}reconstruct_umap_{split}split.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like these two representations a translated vertically by different amounts, or flipped along horizontal axis. TODO: figure out a way (if there is one) to have them in the same line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 3\n",
    "pca_pred = PCA(n_components=n_components)\n",
    "pca_raw = PCA(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pred.fit(pred_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_raw.fit(raw_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_reduced = pca_pred.transform(pred_rec)\n",
    "raw_reduced = pca_raw.transform(raw_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=pred_reduced.shape[1], ncols=1, sharex=True, sharey=False)\n",
    "fig.set_figwidth(25)\n",
    "fig.set_figheight(3 * pred_reduced.shape[1])\n",
    "\n",
    "# batch_size, num_voxels, num_tokens, time_patch_preds = pred_logits.shape\n",
    "\n",
    "# --- Plot Figure ---#\n",
    "for row_idx in range(pred_reduced.shape[1]):\n",
    "    ax = axes[row_idx]\n",
    "    data_parcels = list(range(pred_reduced.shape[0]))\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        pred_reduced[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Masked Predictions\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        data_parcels,\n",
    "        raw_reduced[:, row_idx],\n",
    "        marker=\".\",\n",
    "        markersize=3,\n",
    "        label=\"Input Data\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.set_title(\"PCA Coord {}\".format(row_idx + 1))\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(0.9, 0.93))\n",
    "plt.tight_layout(rect=[0.03, 0.03, 0.95, 0.93])\n",
    "fig.supxlabel(\"Timepoint\")\n",
    "fig.supylabel(\"PCA Value\")\n",
    "plt.suptitle(\"Ground Truth Signal vs Masked Prediction\\n({} Split)\".format(split))\n",
    "plt.savefig(f\"{dir_name}reconstruct_pca_whole_recording_{split}split.png\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 1) Average of a batch of recordings, 2) average of all recordings in a dataset split, 3) average of all recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
